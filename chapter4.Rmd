# 4 Clustering and classification


### 4.1.1 Loading necessary libraries
```{r}
library(MASS)  # For the Boston dataset
library(ggplot2)  # For graphical representations
library(caret)  # For data splitting and preprocessing
library(corrplot)

```
### 4.1.2 Loading the data and investigating the data structure
```{r}
# Step 1: Load and Explore the Boston Dataset
data("Boston")
# Exploring the structure and dimensions of the dataset
```


```{r}
str(Boston)
```


```{r}
dim(Boston)

```

This dataset contains housing values in the suburbs of Boston, has 506 rows and 14 columns, all numerical, chas can be considered categorical as it can only be 0 and 1.
Each row represents a different suburb. Columns are various features like crime rate, number of rooms, age of the housing, etc. More complete description can be found [here](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html)

The variables have the foillowing decriptions:

- CRIM - per capita crime rate by town
- ZN - proportion of residential land zoned for lots over 25,000 sq.ft.
- INDUS - proportion of non-retail business acres per town.
- CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)
- NOX - nitric oxides concentration (parts per 10 million)
- RM - average number of rooms per dwelling
- AGE - proportion of owner-occupied units built prior to 1940
- DIS - weighted distances to five Boston employment centres
- RAD - index of accessibility to radial highways
- TAX - full-value property-tax rate per $10,000
- PTRATIO - pupil-teacher ratio by town
- B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
- LSTAT - % lower status of the population
- MEDV - Median value of owner-occupied homes in $1000's


### 4.1.3 Graphical Overview and Summary of Variables

```{r}
summary(Boston)
```
All the statistics for each variable is not directly comparable, as is expected for real-world data. We can now the relations between the variables to look deeper into the data. 

```{r, echo =F}
ggpairs(Boston,progress = F)
```

- The distributions are skewed: nox and dis have skewed to low values, age to high values. The proportion of non-retail business acres per town, has a distribution with two peaks, as does the tax variable.


- All the variables appear to be correlated. Only 8 pairs are not significantly correlated, all connected to the "categorical variable" Charles River. Most of the correlation make sense: e.g. nox concentrations and the  distance to the city are negatively correlated, but nox and industrialisation are positively correlated.

- Crime rate appears to have a statistically significant correlation all of vars, but chas. Seems to correlate negatively with 5 variables, and positively with 7 variables.

We can plot the correlation in a more visually pleasing way. All the relations between the variables can be seen clearly on this figure
```{r}

cor_matrix <- cor(Boston) 

corrplot(cor_matrix)
```
All the relations between the variables can be seen clearly on this figure




### 4.1.4 Standardising the dataset

As the data described very different phenomena, the values were not directly comparable. E.g. mean value for tax was 408, while nox was ~ 0.55. In order to eliminate that we can standardise the data.

```{r}

scaled_Boston <- as.data.frame(scale(Boston))
summary(scaled_Boston)

```

Now all the means are the same - 0. This standardization makes variables more comparable and often improves machine learning model performance.

#### 4.1.4.1 Creating a categorical variable from scaled crime rate

Using quantiles as breakpoints, and removing the old crime rate variable.

```{r}
quantiles <- quantile(scaled_Boston[, "crim"], probs = c(0, 0.25, 0.5, 0.75, 1))
scaled_Boston$categorical_crim <- cut(scaled_Boston[, "crim"], breaks = quantiles, 
                               include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))
scaled_Boston <- scaled_Boston[,-which(names(Boston) == "crim")]

```

Splitting the Dataset into Train and Test Sets

80% of the data is now in the training set, and the remaining 20% is in the test set.

```{r}
set.seed(123) # For reproducibility
indexes <- createDataPartition(scaled_Boston$categorical_crim, p = 0.8, list = FALSE)
train_set <- scaled_Boston[indexes, ]
test_set <- scaled_Boston[-indexes, ]
```

### 4.1.5 Fitting the linear discriminant analysis on the train set

```{r}
# Loading necessary library
library(MASS)  # For lda function
library(ggplot2)  # For creating biplot

# Step 1: Fit the Linear Discriminant Analysis Model
# Fitting LDA with categorical crime rate as target variable
lda_model <- lda(categorical_crim ~ ., data = train_set)

# Step 2: Summary of the LDA Model
lda_model
```


```{r}

lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  graphics::arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
# The biplot visualizes the linear discriminants (LD1 and LD2) and shows
# how the observations in the training set are separated based on the
# categorical crime rate.
classes <- as.numeric(train_set$categorical_crim)

plot(lda_model, dimen = 2, col = classes,pch = classes)
lda.arrows(lda_model, myscale = 2)

```

The LD1 is influences mostly by rad, the LD2 is influeneces by zn and nox in similar marnitude, but different direction.

### 4.1.6 Fitting the linear discriminant analysis on the train set


```{r}
# Step 6: Save Crime Categories and Update Test Set
# Saving the crime categories from the test set
test_crime_categories <- test_set$categorical_crim

# Removing the categorical crime variable from the test set
test_set <- test_set[,-which(names(test_set) == "categorical_crim")]

# Step 7: LDA Model Prediction
# Fit LDA model on the training set
library(MASS) # LDA is in the MASS package
fit_lda <- lda(categorical_crim ~ ., data = train_set)

# Predicting on the test set
predictions <- predict(fit_lda, newdata = test_set)

```
`table`, `prop.table` and `addmargins`
```{r}

table(Predicted = predictions$class, Actual = test_crime_categories) %>% 
  addmargins()

```

The model is reasonably good, especially at the high and low values, and can be used to differentiate high and low groups. The middle groups become muddled.

### 4.1.7 K-means clustering

Reloading the dataset, scaling and calculating the distances:


```{r}
data(Boston)

Boston_scaled <- data.frame(scale(Boston))

dist_euc <- dist(Boston_scaled)
summary(dist_euc)
```
Mean distance is 4.7.
In order to determine the optimal number of clusters we will look at the total of within cluster sum of squares (WCSS) behaves when the number of cluster changes. We can perform clustering with different k numbers and then look at what happens to the WCSS when the k number is increased. If the value drops rapidly - the k number is good. But the larger the k the harder it may be to interpret the results. So we can start checking and plotting k from 2 to 20, jst to see what happens.



```{r}

k_max <- 20

twcs <- sapply(1:k_max, function(k){kmeans(Boston_scaled, k)$tot.withinss})

elbow_data <- data.frame(
  k = 1:k_max,
  twcs = twcs
)
elbow_plot <- ggplot(elbow_data, aes(x = k, y = twcs)) +
  geom_line() +  # Connect points with lines
  geom_point() +  # Add points
  scale_x_continuous(breaks = 1:k_max) +  # Ensure k values are treated as continuous
  labs(x = "Number of Clusters (k)", y = "Total Within-Cluster Sum of Squares", 
       title = "Determining Optimal k") +
  theme_minimal()

elbow_plot
```

The plot changes the slope quite quickly at k = 2, so we can use this clustering for the further analysis.

```{r}
Boston_scaled_km <- kmeans(Boston_scaled, 2)
```

```{r}
pairs(Boston_scaled,col = Boston_scaled_km$cluster)
```


```{r}
pairs(Boston_scaled[,c(1,10)],col = Boston_scaled_km$cluster)
```

There seems to be good cluster separation between crime and tax.

```{r}
pairs(Boston_scaled[,c(3,5)],col = Boston_scaled_km$cluster)
```
An even better separation for indus and nox. High indus and high nox cluster, and cluster of both low values.

```{r}
pairs(Boston_scaled[,c(2,3)],col = Boston_scaled_km$cluster)
```
There seems also to be good separation for industry and zn variables, two clusters: low indus and high zn, and vice versa.
As we saw earlier the chosen variables have high correlation, and logically should also be correlated. So these results make sense.

### 4.1.8* Bonus. Visualising clustering results with a biplot:

The elbow plot shows that the last point with decreased WCSS is 6, so I decided to look what k=9 clustering looks like. And redoing the LDA using the cluster as the target classes.


```{r}
set.seed(8) # For reproducibility

data("Boston")

Boston_scaled <- as.data.frame(scale(Boston))


Boston_scaled_km <- kmeans(Boston_scaled, centers = "6")


lda.fit <- lda(Boston_scaled_km$cluster ~ . , data = Boston_scaled)

lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  graphics::arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(Boston_scaled_km$cluster)

# plot the lda results (select both lines and execute them at the same time!)
plot(lda.fit, dimen = 2,col = classes,pch = classes)
lda.arrows(lda.fit, myscale = 2)
```
As we can see, the cluster 3 is saparate from all the othersm and the Charles River variable is the main determinant for this cluster. 
Cluster 5 is separate from others, this separation is dependent on the radial roads variable.


### 4.1.9* Bonus. Trainig data that you used to fit the LDA and visualisation:

Installing/loading plotly
```{r}
if (!require(plotly) == T) {
  install.packages("plotly")
}
library(plotly)


```


```{r}
lda.fit <- lda(categorical_crim ~ . , data = train_set)


model_predictors <- dplyr::select(train_set, -categorical_crim)
# check the dimensions
dim(model_predictors)
dim(lda.fit$scaling)
# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)
```


```{r}

plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, color = train_set$categorical_crim, type= 'scatter3d', mode='markers')
```

```{r}

plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, color = factor(Boston_scaled_km$cluster[indexes]), type= 'scatter3d', mode='markers')
```

The crime colouring shows that high crime group is clustered separately, with some med-high groups. The k-means clustering colouring shows clusters 3 and 4 to be together, as in 2d plain, however the 3rd group is split between the two clusters.


## 4.2 Data wrangling for the next weekâ€™s data!

The R script transforming the data for the next week asignment is in the [repository](https://github.com/crastr/IODS-project/blob/master/data/create_human.R), with the `human.csv` file in the same [directory](https://github.com/crastr/IODS-project/tree/master/data)
